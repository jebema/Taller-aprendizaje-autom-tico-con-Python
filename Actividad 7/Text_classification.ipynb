{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic4_occAAiAT"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ioaprt5q5US7"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "yCl0eTNH5RS3"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Text classification with movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKY4XMc9o8iB"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/basic_text_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/tutorials/keras/basic_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/r2.0rc/site/en/tutorials/keras/basic_text_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "En este cuaderno se clasifican las evaluaciones de usuarios a películas como *positivas* o *negativas*. Este es un ejeplo de una clasificación *binaria* o de dos clases, un problema ampliamente aplicable en el aprendizaje automático.\n",
    "\n",
    "Se usará el conjunto de datos de [IMDB](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) que contiene el texto de 50,000 evaluaciones de películas de la Internet Movie DataBase (IMBD). Este conjunto se divide en 25,000 evaluaciones para el entrenamiento y 25,000 para la prueba. Los conjuntos de entrenamiento y prueba estan *balanceados*, lo que significa que ambos contienen la misma cantidad de evaluaciones positivas y negativas.\n",
    "\n",
    "Este cuaderno usa [tf.keras](https://www.tensorflow.org/guide/keras), una API de alto nivel para entrenar modelos en TensorFlow. Para un tutorial más avanzado en clasificación de texto usando `tf.keras`, ve a [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JL-LtD0CBSZR"
   },
   "outputs": [],
   "source": [
    "# keras.datasets.imdb is broken in 1.13 and 1.14, by np 1.16.3\n",
    "!pip install tf_nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Descargar el conjunto de datos IMDB\n",
    "\n",
    "El conjunto de datos IMDB viene incluido con TensorFlow. Ya ha sido preprocesado de tal forma que las evaluaciones (secuencias de palabras) se han convertido a secuencias de enteros, donde cada entero representa una palabra específica en un diccionario.\n",
    "\n",
    "El siguiente código descarga el conjunto de datos IMDB a tu máquina (o usa una copia del caché si ya lo descargaste):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odr-KlzO-lkL"
   },
   "source": [
    "El argumento `num_words=10000` mantiene las 10,000 palabras más frecuentes que ocurren en el conjunto de datos de entrenamiento. Las palabras raras se descartan para mantener manejabla el tamaño de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Exploración de los datos\n",
    "\n",
    "Nos tomaremos un momento para entender el formato de los datos. el conjunto de datos esta preprocesado: cada ejemplo en el arreglo de enteros representa a las palabras de la evaluación de las películas. Cada etiqueta es un valor entero ya se de 0 o 1, donde 0 es una evaluación negativa y 1 es una evaluación positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8qCnve_-lkO"
   },
   "outputs": [],
   "source": [
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnKvHWW4-lkW"
   },
   "source": [
    "El texto de las evaluaciones se ha convertido a enteros, donde cada entero representa una palabra específica en un diccionario. Así es como se ve la primera evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIE4l_72x7DP"
   },
   "source": [
    "Las evaluaciones de las películas pueden tener distinta cantidad de palabras. En el código de abajo se muestra el número de palabras en la primera y segunda evaluación. \n",
    "Ya que las entradas a las redes neuronales deben tener el mismo tamaño, tendremos que resolver esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-6Ii9Pfx6Nr"
   },
   "outputs": [],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wJg2FiYpuoX"
   },
   "source": [
    "### Convertir los enteros de nuevo a palabras\n",
    "\n",
    "Puede ser útil convertir los enteros nuevamente a texto. En el siguiente código creamos una función de ayuda para buscar las palabras en un diccionario que contiene los enteros para mapearlos al texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tr5s_1alpzop"
   },
   "outputs": [],
   "source": [
    "# Un diccionario que mapea las palabras a un índice entero\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Los primeros índices están reservados\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3CNRvEZVppl"
   },
   "source": [
    "Ahora podemos usar la función `decode_review` para desplegar el texto para la primera evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_OqxmH6-lkn"
   },
   "outputs": [],
   "source": [
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFP_XKVRp4_S"
   },
   "source": [
    "## Preparar los datos\n",
    "\n",
    "Los arreglos con las evaluaciones que contienen los enteros deben de convertirse a tensores antes de usarse para introducirse a las redes neuronales. Esta conversión se puede hacer de dos formas:\n",
    "\n",
    "* Convertir los arreglos en vectores con 0s y 1s que indican que palabras de un diccionario ocurren en una evaluación, similar a lo que se conoce como one-hot enoding. Por ejemplo, la secuencia [3, 5] se convertería un vector con 10,000 elementos que estaría lleno de ceros, excepto en los índices 3 y 5 que se refiere a las palabras que ocurren. Entonces, esto sería la primera capa de la red neuronal. Esta forma requiere bastante memoria, ya que requiere una matriz de `num_words * num_reviews`.\n",
    "\n",
    "* Alternativamente, podemos hacer padding (o rellenar) los arreglos de forma que todos tengan la misma longitud, después crear un tensor de enteros con una forma `max_length * num_reviews`. Podemos usar una capa embebida capaz de manejar esta forma como la primer capa de la red.\n",
    "\n",
    "En este tutorial, usaremos la segunda forma.\n",
    "\n",
    "\n",
    "Ya que las evaluaciones de las películas deben tener la misma longitud, usaremos la función [pad_sequences](https://keras.io/preprocessing/sequence/#pad_sequences) para estandarizar las longitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jQv-omsHurp"
   },
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO5MBpyQdipD"
   },
   "source": [
    "Vamos a ver la longitud de los ejemplos ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USSSBnkE-lky"
   },
   "outputs": [],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJoxZGyfjT5V"
   },
   "source": [
    "E inspeccionar la primera evaluación (ahora rellenada):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG8X9cqi-lk9"
   },
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Construir el modelo\n",
    "\n",
    "La red neuronal se crea al apilar capas, lo que requiero dos decisiones de arquitectura principales:\n",
    "\n",
    "* ¿Cuántas capas se usarán en el modelo?\n",
    "* ¿Cuántas *hidden units* (unidades ocultas) se usarán en cada capa?\n",
    "\n",
    "En este ejemplo, los datos de entrada consisten en un arreglo con índices de palabras. Las etiquetas para la predicción son ya sea 0 o 1. Vamos a construir un modelo para este problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD"
   },
   "outputs": [],
   "source": [
    "# la forma de la entrada es el contador del vocabulario usado para las evaluaciones de las pelícuas (10,000 palabras)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PbKQ6mucuKL"
   },
   "source": [
    "Las capas estan apiladas de forma secuencial para construir al clasificador:\n",
    "\n",
    "1. La primera capa es una capa `Embebida`. Esta capa toma el vocabulario de enteros codificados y busca el vector embebido para cada indice de palabra. Estos datos se aprende conforme el modelo se entrena. Los vector agregan una dimensión al arreglo de salida. La dimensión resultante es:>\n",
    "\n",
    "1. The layers are stacked sequentially to build the classifier:`(batch, sequence, embedding)`.\n",
    "2. A una capa llamada `GlobalAveragePooling1D`  regresa un vector de salida con un tamaño fijo para caad ejemplo, promediando sobre la dimensión de la secuencia. Esto permite al modelo la capacidad de manipular la longitud de la variable de entrada, de la manera más fácil posible.\n",
    "3. Este vector de tamaño fijo se conduce a través de una capa completamente conectada (`Dense`) con 16 unidades escondidas.\n",
    "4. La última capa es una capa densamiente conectada con solo un nodo de salida. Por medio de la función de activación `sigmoid`, este valor fluctúa entre 0 y 1, representando una probabilidad o nivel de confianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0XMwnDOp-llH"
   },
   "source": [
    "### Unidades ocultas\n",
    "\n",
    "El modelo tiene dos capas intermedias \"ocultas\", entre las capas de entrada y de salida. el número de salidas (unidades, nodos, o neuronas) es la dimensión del espacio de representación para cada capa. En otras palabras, la cantidad de libertad de la red esta permitida cuando se aprende una representación interna.\n",
    "\n",
    "Si un modelo tiene más unidades ocultas (un espacio de representación de dimensión mas alta), u otro con mas capas. entonces la red puede aprender representaciones más complejas. Sin emabrgo, esto provoca que la red requiere más recursos computacionales y podría llegar a aprender patrones no deseados que mejoran el desempeño en los datos de entrenamiento pero no en los datos de prueba, a esto se le conoce como *overfitting* y se verá mas adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### Optimizador y función de pérdida\n",
    "\n",
    "Un modelo necesita una función de pérdida y un optimizador para el entrenamiento. Ya que es un problema de clasificación binaria, y el modelo entrega una probabilidad como salida (una capa con un solo nodo con una función de activación sigmoide(, usaremos la función de pérdida `binary_crossentropy`.\n",
    "\n",
    "Este no es la única decisión respecto a la función de pérdida, se puede, por ejemplo, escoger `mean_squared_error`. Pero, generalmente la `binary_crossentropy` es mejor para tratar con probabilidades, ya que mide la \"distancia\"entre las distribuciones de probabilidad, o en nuestro caso, entre la distribución de nuestro \"ground-truth\" y las predicciones.\n",
    "\n",
    "Ahora, configuramos el modelo para usar un optimizador y una función de pérdida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCWYwkug-llQ"
   },
   "source": [
    "## Creación de un conjunto de validación\n",
    "\n",
    "Cuando entrenamos, queremos checar la exactitud del modelo en datos que no ha visto nunca. Creamos un *conjunto de validación* al separar 10,000 ejemplos del conjunto de datos de entrenamiento original. (Porqué no solo usar el conjunto de prueba de una vez? Nuestra meta es desarrollar y ajustar nuestro modelos usando únicamente datos de entrenamiento, y después usar datos de prueba solo una vez para evaluar nuestra exactitud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NpcXY9--llS"
   },
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "Entrar el modelo por 40 époas en mini-lotes de 512 ejemplos. Esto es, 40 iteraciones sobre todas las muestras de tensores `x_train` y `y_train`. Mientras se entrena, se monitorea la pérdida y exactitud en las 10,000 muestras del conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Evaluar el modelo\n",
    "\n",
    "Veamos como se desempeña el modelo. Se regresan dos valores. La pérdida (un número que representa nuestro error, valores más bajos son mejores), y exactitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOMKywn4zReN"
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1iEXVTR0Z2t"
   },
   "source": [
    "Esta propuesta, que es algo ingenua, logra una exactitud de 87%. Con propuestas de arquitecturas más avanzadas, el modelo puede incrementar a cerca de 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## Crear un gráfica de exactitud y pérdida a través del tiempo\n",
    "\n",
    "`model.fit()` regresa un objeto `History` que contiene un diccionario con todo lo que sucedió durante el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcvSXvhp-llb"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRKsqL40-lle"
   },
   "source": [
    "Hay cuantro entradas: una para cada medida monitoreada durante el entrenamiento y validación. Esamos estos valores para graficar y comparar la pérdida y exactitud en el entrenamiento y validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGoYf2Js-lle"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hXx-xOv-llh"
   },
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFEmZ5zq-llk"
   },
   "source": [
    "En esta gráfica, los puntos representan la pérdida y la exactitud en el entrenamiento mientras que la linea sólida representa la pérdida y exactitud en la validación.\n",
    "\n",
    "Nota que la pérdida *disminuye* en cada época durante el entrenamiento y la exactitud *incremementa* con cada época. Esto es esperado la que cuando se usa el algoritmo de optimización del descenso del grandiente se debe minimizar la cantidad deseada sobre cada iteración.\n",
    "\n",
    "Esto no es el caso para la périda y la exactitud en la validación, ya que se ven que llegan a un pico en las 20 épocas. Este es un ejemplo de *overfitting*: el modelo funciona mejor en los datos de entremaiento que en datos que no ha visto nunca. Después de este punto, el modelo sobre-optimiza y encuentra representaciones *específicas* para los datos de entrenamiento que no *generalizan* a los datos de prueba.\n",
    "\n",
    "Para este caso particular, podemos prevenir el *overfitting* al parar el entrenamiento después de las 20 épocas.\n",
    "\n",
    "## Predecir una evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(test_data[[1]])\n",
    "print(\"Probability: \", results)\n",
    "print(\"Label predicted: \",test_labels[1])\n",
    "print(\"Evaluación:\")\n",
    "decode_review(test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar palabras del diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(word_index.keys())[0:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic-text-classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
